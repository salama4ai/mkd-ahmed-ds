{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "xcccEva1WWrh",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pipeline\n",
      "  Downloading pipeline-0.1.0-py3-none-any.whl (2.6 kB)\n",
      "Installing collected packages: pipeline\n",
      "Successfully installed pipeline-0.1.0\n"
     ]
    }
   ],
   "source": [
    "# !pip install sentence_transformers openai\n",
    "# !pip install pinecone-client\n",
    "# !pip install docx2txt\n",
    "# !pip install pipeline\n",
    "\n",
    "# # !pip3 install git+https://git@github.com/pinecone-io/pinecone-python-client.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "BYUc4Z7vY2bb"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['OPENAI_API_KEY'] = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "MZAy8TaKY6pI",
    "tags": []
   },
   "outputs": [],
   "source": [
    "#This is for embedding. In here, one LM model from huggingface used.\n",
    "\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "\n",
    "text ='Abc'\n",
    "# model.encode(text).tolist() #exmple how to do encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "gu04tON0cZvT"
   },
   "outputs": [],
   "source": [
    "#Function to split long documents in to smaller parts\n",
    "def split_text_into_chunks(plain_text, max_chars=2000):\n",
    "    \"\"\"convert text into chunks\"\"\"\n",
    "    \n",
    "    text_chunks = []\n",
    "    current_chunk = \"\"\n",
    "    for line in plain_text.split(\"\\n\"):\n",
    "        if len(current_chunk) + len(line) + 1 <= max_chars:\n",
    "            current_chunk += line + \" \"\n",
    "        else:\n",
    "            text_chunks.append(current_chunk.strip())\n",
    "            current_chunk = line + \" \"\n",
    "    if current_chunk:\n",
    "        text_chunks.append(current_chunk.strip())\n",
    "    return text_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "EqCYELlQZN0m"
   },
   "outputs": [],
   "source": [
    "import pinecone\n",
    "pinecone.init(api_key=\"dae36959-08bc-4f9e-8468-3741321386b6\", environment=\"us-east-1-aws\") #Todo: Initialization of vector database module\n",
    "index = pinecone.Index(\"squad\") #Todo: Fill out with index name.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# import pinecone from sentence_transformers\n",
    "# import SentenceTransformer from transformers\n",
    "# import pipeline\n",
    "\n",
    "# pinecone.init(api_key=\"dae36959-08bc-4f9e-8468-3741321386b6\", environment=\"us-east-1-aws\")\n",
    "# index = pinecone.Index(\"squad\")\n",
    "\n",
    "# # Retrievers embed the context passages and questions\n",
    "# retriever = SentenceTransformer('multi-qa-MiniLM-L6-cos-v1')\n",
    "# # Readers extract the answer from the returned context\n",
    "# reader = pipeline(\n",
    "#   tokenizer='deepset/electra-base-squad2',\n",
    "#   model='deepset/electra-base-squad2',\n",
    "#   task='question-answering'\n",
    "# )\n",
    "# question = \"How much oil does Egypt produce in a day?\"\n",
    "# embed = retriever.encode([question]).tolist()\n",
    "# # search for context passage with the answer\n",
    "# matches = index.query(embed, top_k=5, include_metadata=True)[\"matches\"]\n",
    "# # extract the answer from the context passages\n",
    "# for m in matches:\n",
    "#   answer = reader(question=question, context=m[\"metadata\"][\"context\"])\n",
    "#   print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "CAqSjLcQZjjJ"
   },
   "outputs": [],
   "source": [
    "def addData(corpusData):\n",
    "    \"\"\"encode all the splitted chunks and add them to the vector database\"\"\"\n",
    "    \n",
    "    id  = index.describe_index_stats()['total_vector_count']\n",
    "    for i in range(len(corpusData)):\n",
    "        chunk=corpusData[i]\n",
    "        chunkInfo=(str(id+i),\n",
    "                model.encode(chunk).tolist(), #We are using the model to encode the original chunk of text.\n",
    "                {'context': chunk}) #In metadata we are storing the original text here as context. \n",
    "        index.upsert(vectors=[chunkInfo])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import docx2txt\n",
    "\n",
    "docx_file = \"./DataLaw.docx\"\n",
    "\n",
    "def index_populating(docx_file):\n",
    "    \"\"\"read docx file and call split_text_into_chunks function to split it into chunks then \n",
    "    call addData() to add it to the index\"\"\"\n",
    "    \n",
    "    # extract text from docx file\n",
    "    plain_text = docx2txt.process(docx_file)\n",
    "    # # extract text and write images in /tmp/img_dir\n",
    "    # text = docx2txt.process(\"file.docx\", \"/tmp/img_dir\") \n",
    "    \n",
    "    # split plain_text into chunks\n",
    "    corpusData = split_text_into_chunks(plain_text)\n",
    "    # encode chuncks and index it\n",
    "    addData(corpusData)\n",
    "    \n",
    "index_populating(docx_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8VIZ5_ufbRQ5"
   },
   "outputs": [],
   "source": [
    "#This function is responsible for matching the input string with already existing data on vector database.\n",
    "\n",
    "def find_match(query,k):\n",
    "    \"\"\"This function is responsible for matching the input string\n",
    "    with already existing data on vector database\"\"\"\n",
    "\n",
    "    # encode the query\n",
    "    query_em = model.encode(query).tolist()\n",
    "    result = index.query(query_em, top_k=k, includeMetadata=True)\n",
    "    \n",
    "    return [result['matches'][i]['metadata']['context'] for i in range(k)]\n",
    "\n",
    "context = find_match(query,k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "AoRDzK85aF9E"
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (Temp/ipykernel_6428/909643776.py, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"C:\\Users\\huzyfa\\AppData\\Local\\Temp/ipykernel_6428/909643776.py\"\u001b[1;36m, line \u001b[1;32m2\u001b[0m\n\u001b[1;33m    import SentenceTransformer from transformers\u001b[0m\n\u001b[1;37m                               ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "from transformers import pipeline\n",
    "\n",
    "model_name = \"gpt3\"\n",
    "# load the reader model into a question-answering pipeline\n",
    "reader = pipeline(tokenizer=model_name, model=model_name, task=\"question-answering\")\n",
    "\n",
    "def create_prompt(context,query):\n",
    "    #Todo: Should be generated with the context/contexts we find by doing semantaic search]\n",
    "    \"\"\" extracts answer from the context passage\"\"\"\n",
    "    \n",
    "    results = []\n",
    "    for c in context:\n",
    "        # feed the reader the question and contexts to extract answers\n",
    "        answer = reader(question=query, context=c)\n",
    "        # add the context to answer dict for printing both together\n",
    "        answer[\"context\"] = c\n",
    "        results.append(answer)\n",
    "    # sort the result based on the score from reader model\n",
    "    sorted_result = pprint(sorted(results, key=lambda x: x[\"score\"], reverse=True))\n",
    "    return sorted_result\n",
    "\n",
    "prompt = create_prompt(context,query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "IyPNrKW3aeoD"
   },
   "outputs": [],
   "source": [
    "def generate_answer(prompt):\n",
    "    #Todo: Pass the generated prompt and pass it to gpt-3 to get answers.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uWM2IcOKarWz"
   },
   "outputs": [],
   "source": [
    "def user_query(query):\n",
    "    #Todo: Make all the things together.\n",
    "    context = find_match(query,k)\n",
    "    prompt = create_prompt(context,query)\n",
    "    answer = generate_answer(prompt)\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jBds94_gbJ_G"
   },
   "outputs": [],
   "source": [
    "user_query(\"How can I do this?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
